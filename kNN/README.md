关于k近邻算法(kNN)：
  
首先，这是一个监督学习算法。也就是说，我们需要的是有标签的数据。  
为了判断一个新的数据点 (假设该点为`p`) 归属于哪一个类，我们就观察一下距离点`p`最近的`k`个样本点，看看这`k`个样本分别属于哪一个类别，最后我们统计一下每个类别的样本个数，然后把点`p`指定为样本个数最多的那个类别。
>假设`k=5`，也就是说，为了判断点`p`的类别，我们需要观察一下距离它最近的其他5个样本点的类别归属。假设这五个邻居中，三个属于类别`A`，两个属于类别`B`，则点`p`被划分到`A`类。

一般来说，`k`取值越大，对outliers或者说noise的承受能力就越强。
