1. 什么是随机森林？  
   随机森林就是一堆决策树的集合。

   **那么这堆决策树是怎么产生的呢**？
   
   首先，从训练数据中，有放回地采样N次，比如，训练数据一共有100行，我要建立一个有3棵树的森林，则我一共采样3次数据。第一次，从100行中随机抽取30行(30是随便说的一个数字，其实多少都行)，利用这30行数据建立一个决策树。然后第二次，我还从100行中随机抽取30行，由于是有放回的抽取，所以我们可能会抽取到一些和第一次重复的数据，不过这个没关系。然后我们再建立一个决策树。然后第三次，我们依然从100行中随机抽取30行，然后建立一个决策树。这3个决策树的集合，就是一个随机森林。当我们使用这个随机森林进行预测的时候，我们会分别得到3棵决策树给出的不同的结果，然后我们将其平均，作为这个随机森林的预测结果并输出。

   有一点可以注意一下，我们在上面的例子中，从100行中随机抽取30行，如果抽取的更多或者抽取的更少有什么影响呢？如果抽取的更多，则我们使用到重复数据的概率会增大，也就会导致，最后得到的3棵决策树差别不会很大，我们称这些决策树之间是highly correlated. 相反，如果我们抽取的更少，比如只抽取10行，则有可能完全没有重复数据，因此大概率我们最后得到的3棵决策树差别也会很大，我们称这些决策树之间是low correlated或者说uncorrelated.

2. 什么是OOB (out-of-bag) error？  
   这是专属于随机森林这一类算法的概念，继续往下读你就知道为什么了。

   当你的数据较少时，如果还要单独划分出一部分来做validation set，那么剩下的部分作为training set可能会不足够。这个时候怎么办呢？回想一下我们随机森林的构建过程。在构建的时候，我们每次从所有数据中采样一部分来建立决策树，假设，我们从100行数据中，随机抽取30行来构建决策树，那么剩下的70行，对于这一棵决策树来说，就没有被用到。我们可以将这剩下的70行作为这棵树的validation set。同理，每一棵决策树在构建时没有被用到的那部分数据，也都可以作为那棵决策树的validation set。而通过这种方式获得的validation set，在评价随机森林的性能的时候，计算出来的error，就叫OOB error。
